import { Pinecone } from '@pinecone-database/pinecone';
import { GoogleGenAI } from '@google/genai';

export interface MemoryItem {
  id: string;
  content: string;
  metadata: {
    timestamp: number;
    type: 'conversation' | 'document' | 'note' | 'conversation_summary';
    source?: string;
    userId?: string;
    chatId?: string;           // ðŸŽ¯ NEW! Chat-scoped memory optimization
    tags?: string[];
    sessionId?: string;
    turnCount?: number;
    timespanStart?: number;
    timespanEnd?: number;
    isFirstMessage?: boolean;  // ðŸŽ¯ NEW! Track if this is chat's first message
  };
}

export interface SearchResult extends MemoryItem {
  score: number;
}

export class EmbeddingService {
  private pinecone: Pinecone;
  private ai: GoogleGenAI;
  private indexName: string;

  constructor() {
    if (!process.env.PINECONE_API_KEY) {
      throw new Error('PINECONE_API_KEY is required');
    }
    if (!process.env.GEMINI_API_KEY) {
      throw new Error('GEMINI_API_KEY is required');
    }
    if (!process.env.PINECONE_INDEX_NAME) {
      throw new Error('PINECONE_INDEX_NAME is required');
    }

    this.pinecone = new Pinecone({
      apiKey: process.env.PINECONE_API_KEY,
    });
    this.ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
    this.indexName = process.env.PINECONE_INDEX_NAME;
  }

  /**
   * Generate embeddings using Google's text-embedding model
   * Using text-embedding-004 which produces 768 dimensions
   * If your Pinecone index has 1024 dimensions, you'll need to recreate it with 768
   */
  async generateEmbedding(text: string): Promise<number[]> {
    try {
      const response = await this.ai.models.embedContent({
        model: 'text-embedding-004',
        contents: [{ parts: [{ text }] }]
      });

      if (!response.embeddings?.[0]?.values) {
        throw new Error('No embedding returned from Gemini API');
      }

      const embedding = response.embeddings[0].values;
      console.log(`Generated embedding with ${embedding.length} dimensions`);
      
      return embedding;
    } catch (error) {
      console.error('Error generating embedding:', error);
      throw new Error(`Failed to generate embedding: ${error}`);
    }
  }

  /**
   * Initialize or get the Pinecone index
   */
  async getIndex() {
    try {
      return this.pinecone.index(this.indexName);
    } catch (error) {
      console.error('Error accessing Pinecone index:', error);
      throw new Error(`Failed to access index ${this.indexName}: ${error}`);
    }
  }

  /**
   * Store a memory item in Pinecone
   */
  async storeMemory(memoryItem: MemoryItem): Promise<void> {
    try {
      const embedding = await this.generateEmbedding(memoryItem.content);
      const index = await this.getIndex();

      console.log(`Storing memory with ${embedding.length} dimensions`);

      await index.upsert([
        {
          id: memoryItem.id,
          values: embedding,
          metadata: {
            content: memoryItem.content,
            ...memoryItem.metadata
          }
        }
      ]);

      console.log(`Memory stored with ID: ${memoryItem.id}`);
    } catch (error: any) {
      console.error('Error storing memory:', error);
      
      // Check for dimension mismatch error
      if (error?.message?.includes('dimension') && error?.message?.includes('does not match')) {
        throw new Error(
          `Dimension mismatch: Your Pinecone index expects different dimensions than the embedding model produces. ` +
          `Please recreate your Pinecone index with 768 dimensions for text-embedding-004 model, or ` +
          `update the embedding model to match your index dimensions.`
        );
      }
      
      throw new Error(`Failed to store memory: ${error}`);
    }
  }

  /**
   * Store multiple memory items in batch
   */
  async storeMemories(memoryItems: MemoryItem[]): Promise<void> {
    try {
      const index = await this.getIndex();
      const vectors = [];

      for (const item of memoryItems) {
        const embedding = await this.generateEmbedding(item.content);
        vectors.push({
          id: item.id,
          values: embedding,
          metadata: {
            content: item.content,
            ...item.metadata
          }
        });
      }

      // Batch upsert in chunks of 100 (Pinecone limit)
      const chunkSize = 100;
      for (let i = 0; i < vectors.length; i += chunkSize) {
        const chunk = vectors.slice(i, i + chunkSize);
        await index.upsert(chunk);
      }

      console.log(`${memoryItems.length} memories stored successfully`);
    } catch (error) {
      console.error('Error storing memories:', error);
      throw new Error(`Failed to store memories: ${error}`);
    }
  }

  /**
   * Search for relevant memories based on semantic similarity
   * ðŸŽ¯ OPTIMIZED: Chat-scoped search for 90%+ cost reduction!
   */
  async searchMemories(
    query: string, 
    options: {
      topK?: number;
      threshold?: number;
      filter?: Record<string, any>;
      userId?: string;
      chatId?: string;          // ðŸŽ¯ NEW! Chat-specific search
      isNewChat?: boolean;      // ðŸŽ¯ NEW! Different strategy for new chats
    } = {}
  ): Promise<SearchResult[]> {
    try {
      const { topK = 5, threshold = 0.7, filter, userId, chatId, isNewChat = false } = options;
      
      const queryEmbedding = await this.generateEmbedding(query);
      const index = await this.getIndex();

      // ðŸŽ¯ SMART FILTERING STRATEGY for cost optimization
      let searchFilter = filter || {};
      
      // Strategy 1: NEW CHAT - Search all user's chats (discover cross-chat context)
      if (isNewChat && userId) {
        searchFilter = { ...searchFilter, userId };
        console.log(`ðŸ†• New chat - searching ALL user chats (userId: ${userId}) for context discovery`);
      }
      // Strategy 2: EXISTING CHAT - Search only this specific chat (90% cost reduction!)
      else if (chatId && userId) {
        searchFilter = { ...searchFilter, userId, chatId };
        console.log(`ðŸ’¬ Continuing chat - searching ONLY chatId: ${chatId} (optimized!)`);
      }
      // Strategy 3: Fallback - User-wide search
      else if (userId) {
        searchFilter = { ...searchFilter, userId };
        console.log(`ðŸ” Fallback - searching all chats for user: ${userId}`);
      }

      const searchResponse = await index.query({
        vector: queryEmbedding,
        topK,
        includeMetadata: true,
        ...(Object.keys(searchFilter).length > 0 && { filter: searchFilter })
      });

      const results: SearchResult[] = searchResponse.matches
        ?.filter(match => (match.score || 0) >= threshold)
        .map(match => ({
          id: match.id || '',
          content: (match.metadata?.content as string) || '',
          score: match.score || 0,
          metadata: {
            timestamp: (match.metadata?.timestamp as number) || Date.now(),
            type: (match.metadata?.type as 'conversation' | 'document' | 'note') || 'note',
            source: match.metadata?.source as string,
            userId: match.metadata?.userId as string,
            tags: match.metadata?.tags as string[]
          }
        })) || [];

      console.log(`Found ${results.length} relevant memories for query: "${query}"`);
      return results;
    } catch (error) {
      console.error('Error searching memories:', error);
      throw new Error(`Failed to search memories: ${error}`);
    }
  }

  /**
   * Delete a memory by ID
   */
  async deleteMemory(id: string): Promise<void> {
    try {
      const index = await this.getIndex();
      await index.deleteOne(id);
      console.log(`Memory deleted: ${id}`);
    } catch (error) {
      console.error('Error deleting memory:', error);
      throw new Error(`Failed to delete memory: ${error}`);
    }
  }

  /**
   * Delete memories based on filter criteria
   */
  async deleteMemories(filter: Record<string, any>): Promise<void> {
    try {
      const index = await this.getIndex();
      await index.deleteMany(filter);
      console.log('Memories deleted based on filter:', filter);
    } catch (error) {
      console.error('Error deleting memories:', error);
      throw new Error(`Failed to delete memories: ${error}`);
    }
  }

  /**
   * Get memory statistics
   */
  async getMemoryStats(): Promise<{ totalVectors: number; indexDimension: number }> {
    try {
      const index = await this.getIndex();
      const stats = await index.describeIndexStats();
      
      return {
        totalVectors: stats.totalRecordCount || 0,
        indexDimension: stats.dimension || 0
      };
    } catch (error) {
      console.error('Error getting memory stats:', error);
      throw new Error(`Failed to get memory stats: ${error}`);
    }
  }

  /**
   * Initialize the index if it doesn't exist
   * Note: In practice, you should create the index through Pinecone console
   */
  async ensureIndexExists(): Promise<void> {
    try {
      const indexList = await this.pinecone.listIndexes();
      const indexExists = indexList.indexes?.some(index => index.name === this.indexName);

      if (!indexExists) {
        console.warn(`Index ${this.indexName} does not exist. Please create it in Pinecone console with dimension 768.`);
        throw new Error(`Index ${this.indexName} does not exist. Please create it in Pinecone console.`);
      }

      console.log(`Index ${this.indexName} is ready`);
    } catch (error) {
      console.error('Error checking index existence:', error);
      throw error;
    }
  }
}

// Singleton instance
let embeddingService: EmbeddingService | null = null;

export const getEmbeddingService = (): EmbeddingService => {
  if (!embeddingService) {
    try {
      embeddingService = new EmbeddingService();
    } catch (error) {
      console.error('Failed to initialize EmbeddingService:', error);
      throw error;
    }
  }
  return embeddingService;
};